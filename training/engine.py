import os
from tqdm.auto import tqdm
import warnings
warnings.filterwarnings('ignore', message='Spectral initialisation failed')

import torch
import torch.nn.functional as F
import torch.distributed as dist

from utils import timer

# ë‹¨ì¼ ë°°ì¹˜ í•™ìŠµ í•¨ìˆ˜
def train_batch(args, model, batch, loss_module, device, accelerator, dataset, disable_cxr=False, disable_txt=False, max_length=256,
                bce_weight=None, ce_weight=None, ucl_weight=None
    ):
    
    model.train()

    # ==================== 1. ë°°ì¹˜ ë°ì´í„° GPU ì „ì†¡ ====================
    with timer("Batch Data preparation", accelerator):
        # New multi-task format
        for k in ['edema_labels', 'subtype_labels', 'window_mask', 'valid_seq_mask']:
            batch[k] = batch[k].to(device, non_blocking=True)
        edema_labels = batch['edema_labels']
        subtype_labels = batch['subtype_labels']

        demo_features = batch.get('demo_features')
        if demo_features is not None:
            demo_features = demo_features.to(device, non_blocking=True)

        img_index_tensor = batch['img_index_tensor']
        txt_index_tensor = batch['text_index_tensor']
        has_cxr = (img_index_tensor != -1).long().to(device, non_blocking=True)   # [B, W, T]
        has_text = (txt_index_tensor != -1).long().to(device, non_blocking=True)  # [B, W, T]

        window_mask = batch['window_mask']
        seq_valid_mask = batch['valid_seq_mask']
        stay_ids = torch.tensor(batch['stay_ids'], dtype=torch.long, device=device)  # [B]

    # ==================== 2. ëª¨ë¸ ì…ë ¥ ë°ì´í„° ì „ì²˜ë¦¬ ====================
    with timer("ë°ì´í„° ì „ì²˜ë¦¬ ì‘ì—…", accelerator):
        ts_series, cxr_views, text_series, has_cxr, has_text = prepare_multiview_inputs_v2(
            batch, device, has_cxr, has_text,
            dataset=dataset,
            disable_cxr=disable_cxr,
            disable_txt=disable_txt,
            max_length=max_length,
        )

    # ==================== 3. Forward Pass ë° Loss ê³„ì‚° ====================
    with timer("Batchë³„ Embedding ì¶”ì¶œ ë° Loss ì—°ì‚° ì´", accelerator):
        with accelerator.autocast():
            time_steps = batch.get('time_steps', None)
            if time_steps is not None:
                time_steps = time_steps.to(device, non_blocking=True)

            model_type = type(model).__name__ # ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ forward ë¶„ë¦¬ ìˆ˜í–‰.

            if "Contrastive" in model_type:  # Stage 1: MultiModalContrastiveModel
                model_outputs = model(
                    args, ts_series, cxr_views, text_series, has_cxr, has_text,
                    window_mask, seq_valid_mask, demo_features, time_steps=time_steps
                )

                # Unpack model outputs
                edema_logits = model_outputs['edema_logits']          # [B, W, 1]
                subtype_logits = model_outputs['subtype_logits']       # [B, W, 2]
                valid_embeddings = model_outputs['valid_embeddings']   # [Nwin, 256]
                window_time_indices = model_outputs['window_time_indices']  # [Nwin]
                batch_indices = model_outputs['batch_indices']         # [Nwin]

            with timer("Main loss ì—°ì‚°", accelerator):
                total_batch_loss, bce_loss_t, ce_loss_t, ucl_loss_t = loss_module(
                    edema_logits = edema_logits,
                    subtype_logits = subtype_logits,
                    valid_embeddings = valid_embeddings,
                    window_time_indices = window_time_indices,
                    batch_indices = batch_indices,
                    edema_labels=edema_labels,
                    subtype_labels=subtype_labels,
                    window_mask=window_mask,
                    bce_weight=bce_weight,
                    ce_weight=ce_weight,
                    ucl_weight=ucl_weight,
                    device=device,
                    accelerator=accelerator
                )

    # ==================== 5. Metrics ìˆ˜ì§‘ ====================
    window_count = window_mask.sum().item()
    batch_bce = float(bce_loss_t.detach().item())
    batch_ce = float(ce_loss_t.detach().item())
    batch_ucl = float(ucl_loss_t.detach().item())

    batch_outputs = {
        'edema_labels': edema_labels,
        'subtype_labels': subtype_labels,
        'edema_logits': edema_logits,
        'subtype_logits': subtype_logits
    }

    batch_counts = {
        'window_count': window_count,
    }

    return total_batch_loss, batch_bce, batch_ce, batch_ucl, batch_outputs, batch_counts


def prepare_multiview_inputs_v2(batch, device, has_cxr, has_text, dataset, disable_cxr=False, disable_txt=False, max_length=256):
    """
    Flow:
        1. Time-series: ë‹¨ìˆœ GPU ì „ì†¡ [B, W, T, D]
        2. Text: unique_txt_keys â†’ input_ids, attention_mask êµ¬ì„±
        3. Image: unique_img_paths â†’ ìºì‹œì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ
        4. Positions: ê° unique ë°ì´í„°ê°€ ë°°ì¹˜ì˜ ì–´ë””ì— ìœ„ì¹˜í•˜ëŠ”ì§€ ì €ì¥
    """
    ts = batch['ts_tensor']
    img_index_tensor = batch['img_index_tensor']
    txt_index_tensor = batch['text_index_tensor']
    unique_img_paths = batch['unique_img_paths']
    unique_txt_keys = batch['unique_txt_keys']

    # ==================== TEXT PREPARATION ====================
    if not disable_txt:
        text_ids_list, text_masks_list = [], []
        for stay_id, hour in unique_txt_keys:
            token = dataset.text_map[stay_id][hour]
            ids = torch.tensor(token['input_ids'], dtype=torch.long)
            mask = torch.tensor(token['attention_mask'], dtype=torch.long)

            current_len = len(ids)
            if current_len > max_length:
                ids = ids[:max_length]
                mask = mask[:max_length]
            elif current_len < max_length:
                pad_len = max_length - current_len
                ids = F.pad(ids, (0, pad_len), value=0)
                mask = F.pad(mask, (0, pad_len), value=0)

            text_ids_list.append(ids)
            text_masks_list.append(mask)

        if len(unique_txt_keys) > 0:
            unique_text_ids = torch.stack(text_ids_list, dim=0).to(device, non_blocking=True)
            unique_text_masks = torch.stack(text_masks_list, dim=0).to(device, non_blocking=True)
        else:
            unique_text_ids = torch.empty(0, max_length, dtype=torch.long, device=device)
            unique_text_masks = torch.empty(0, max_length, dtype=torch.long, device=device)
    else:
        unique_text_ids = torch.empty(0, max_length, dtype=torch.long, device=device)
        unique_text_masks = torch.empty(0, max_length, dtype=torch.long, device=device)

    # ==================== IMG PREPARATION ====================
    if not disable_cxr:
        if len(unique_img_paths) > 0:
            unique_imgs = torch.stack(
                [dataset.load_image_cached(path) for path in unique_img_paths],
                dim=0
            ).to(device, non_blocking=True)
        else:
            num_channels = 3 if dataset.to_3ch else 1
            unique_imgs = torch.empty(0, num_channels, 224, 224, device=device)
    else:
        num_channels = 3 if dataset.to_3ch else 1
        unique_imgs = torch.empty(0, num_channels, 224, 224, device=device)

    ts_series = ts.to(device, non_blocking=True)  # [B, W, T, D]

    # Text data structure
    if not disable_txt:
        valid_positions = (txt_index_tensor != -1).nonzero(as_tuple=False)
        if len(valid_positions) > 0:
            unique_indices = txt_index_tensor[valid_positions[:, 0], valid_positions[:, 1], valid_positions[:, 2]]
            text_data = {
                'unique_input_ids': unique_text_ids,
                'unique_attention_mask': unique_text_masks,
                'unique_indices': unique_indices,
                'positions': valid_positions
            }
        else:
            text_data = {
                'unique_input_ids': torch.empty(0, max_length, dtype=torch.long, device=device),
                'unique_attention_mask': torch.empty(0, max_length, dtype=torch.long, device=device),
                'unique_indices': torch.empty(0, dtype=torch.long, device=device),
                'positions': torch.empty(0, 3, dtype=torch.long, device=device)
            }
    else:
        text_data = {
            'unique_input_ids': torch.empty(0, max_length, dtype=torch.long, device=device),
            'unique_attention_mask': torch.empty(0, max_length, dtype=torch.long, device=device),
            'unique_indices': torch.empty(0, dtype=torch.long, device=device),
            'positions': torch.empty(0, 3, dtype=torch.long, device=device)
        }

    # CXR data structure
    if not disable_cxr:
        valid_positions = (img_index_tensor != -1).nonzero(as_tuple=False)
        if len(valid_positions) > 0:
            unique_indices = img_index_tensor[valid_positions[:, 0], valid_positions[:, 1], valid_positions[:, 2]]
            cxr_data = {
                'unique_images': unique_imgs,
                'unique_indices': unique_indices,
                'positions': valid_positions
            }
        else:
            num_channels = 3 if dataset.to_3ch else 1
            cxr_data = {
                'unique_images': torch.empty(0, num_channels, 224, 224, device=device),
                'unique_indices': torch.empty(0, dtype=torch.long, device=device),
                'positions': torch.empty(0, 3, dtype=torch.long, device=device)
            }
    else:
        num_channels = 3 if dataset.to_3ch else 1
        cxr_data = {
            'unique_images': torch.empty(0, num_channels, 224, 224, device=device),
            'unique_indices': torch.empty(0, dtype=torch.long, device=device),
            'positions': torch.empty(0, 3, dtype=torch.long, device=device)
        }

    return ts_series, cxr_data, text_data, has_cxr, has_text


############################################################################################
############################################################################################
# Grave of codes

# 3D
# def plot_umap_3d(args, window_embeddings_list, window_labels_list, window_pred_list=None, epoch=None, prefix=None, pca_model=None):
#     window_embeddings = torch.cat(window_embeddings_list, dim=0)
#     window_labels = torch.cat(window_labels_list, dim=0)
    
#     window_pred = None
#     if window_pred_list is not None:
#         window_pred = torch.cat(window_pred_list, dim=0)
#         window_pred = window_pred.cpu().numpy()

#     window_embeddings = window_embeddings.cpu().numpy()
#     window_labels = window_labels.cpu().numpy()

#     # PCA ì¢Œí‘œê³„ ê³ ì •
#     if pca_model is None:
#         pca_model = PCA(n_components=args.pca_components, random_state=args.random_seed)
#         pca_model.fit(window_embeddings)

#     window_embeddings = pca_model.transform(window_embeddings)

#     # UMAP -> 3D ë³€í™˜
#     umap_model = UMAP(
#         n_components=3,
#         n_neighbors=args.umap_n_neighbors,
#         min_dist=args.umap_min_dist,
#         metric=args.umap_metric,
#         random_state=args.random_seed
#     )

#     embeddings_3d = umap_model.fit_transform(window_embeddings)

#     umap_df = pd.DataFrame({
#         'x': embeddings_3d[:, 0],
#         'y': embeddings_3d[:, 1],
#         'z': embeddings_3d[:, 2],
#         'label': window_labels
#     })

#     if window_pred is not None:
#         umap_df['pred'] = window_pred

#     label_colors = {
#         0: 'lightcoral',
#         1: 'turquoise', 
#         2: 'blueviolet',
#         -1: 'black'  # Unlabeled
#     }

#     label_mapping = {
#         0: 'Negative',
#         1: 'Noncardiogenic Edema',
#         2: 'Cardiogenic Edema',
#         -1: 'Unlabeled'
#     }

#     # Plotly 3D visualization
#     fig = px.scatter_3d(
#         umap_df,
#         x='x', y='y', z='z',
#         color=umap_df['label'].map(label_mapping),
#         color_discrete_map={
#             label_mapping[k]: v for k, v in label_colors.items()
#         },
#         opacity=0.65,
#         title=f"{prefix.title()} 3D UMAP @ Epoch {epoch}" if epoch is not None else f"{prefix.title()} 3D UMAP"
#     )

#     fig.update_traces(marker=dict(size=2, line=dict(width=0)))
#     fig.update_layout(
#         scene=dict(
#             xaxis_title="component 0",
#             yaxis_title="component 1",
#             zaxis_title="component 2"
#         ),
#         legend_title="Classes"
#     )

#     # ì €ì¥
#     save_dir = args.umap_save_dir
#     os.makedirs(save_dir, exist_ok=True)
#     file_base = f"{prefix}_umap_epoch{epoch}" if epoch is not None else f"{prefix}_umap"

#     # HTML (interactive)
#     html_path = os.path.join(save_dir, f"{file_base}.html")
#     fig.write_html(html_path)
#     return pca_model


# def compute_contrastive_metrics(embeddings, labels):
#     """
#     ëŒ€ì¡°í•™ìŠµ í’ˆì§ˆ ì§ì ‘ ì¸¡ì •: Intra-class vs Inter-class distance

#     Args:
#         embeddings: torch.Tensor [N, D] - Embedding vectors
#         labels: torch.Tensor [N] - Class labels

#     Returns:
#         dict with:
#             - intra_dist: ê°™ì€ í´ë˜ìŠ¤ ë‚´ í‰ê·  ê±°ë¦¬ (ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ)
#             - inter_dist: ë‹¤ë¥¸ í´ë˜ìŠ¤ ê°„ í‰ê·  ê±°ë¦¬ (í´ìˆ˜ë¡ ì¢‹ìŒ)
#             - separation_ratio: inter/intra (í´ìˆ˜ë¡ ì¢‹ìŒ, >2.0 ëª©í‘œ)
#     """
#     embeddings_np = embeddings.cpu().numpy()
#     labels_np = labels.cpu().numpy()

#     # Cosine distance matrix
#     dist_matrix = cosine_distances(embeddings_np, embeddings_np)

#     intra_dists = []
#     inter_dists = []

#     unique_labels = np.unique(labels_np)

#     for label in unique_labels:
#         # ê°™ì€ í´ë˜ìŠ¤ ë§ˆìŠ¤í¬
#         same_class_mask = labels_np == label
#         same_class_indices = np.where(same_class_mask)[0]

#         if len(same_class_indices) < 2:
#             continue

#         # Intra-class distance (ê°™ì€ í´ë˜ìŠ¤ ë‚´)
#         for i in range(len(same_class_indices)):
#             for j in range(i+1, len(same_class_indices)):
#                 idx_i, idx_j = same_class_indices[i], same_class_indices[j]
#                 intra_dists.append(dist_matrix[idx_i, idx_j])

#         # Inter-class distance (ë‹¤ë¥¸ í´ë˜ìŠ¤ì™€)
#         diff_class_indices = np.where(~same_class_mask)[0]
#         if len(diff_class_indices) == 0:
#             continue

#         # ìƒ˜í”Œë§ìœ¼ë¡œ ê³„ì‚°ëŸ‰ ê°ì†Œ (ìµœëŒ€ 1000ê°œ í˜ì–´ë§Œ)
#         max_pairs = min(1000, len(same_class_indices) * len(diff_class_indices))
#         sample_size = min(len(same_class_indices), int(np.sqrt(max_pairs)))

#         sampled_same = np.random.choice(same_class_indices, size=min(sample_size, len(same_class_indices)), replace=False)
#         sampled_diff = np.random.choice(diff_class_indices, size=min(sample_size, len(diff_class_indices)), replace=False)

#         for idx_i in sampled_same:
#             for idx_j in sampled_diff:
#                 inter_dists.append(dist_matrix[idx_i, idx_j])

#     intra_mean = np.mean(intra_dists) if len(intra_dists) > 0 else 0.0
#     inter_mean = np.mean(inter_dists) if len(inter_dists) > 0 else 0.0
#     ratio = inter_mean / intra_mean if intra_mean > 1e-6 else 0.0

#     return {
#         'intra_dist': intra_mean,
#         'inter_dist': inter_mean,
#         'separation_ratio': ratio
#     }

# def compute_class_compactness(embeddings, labels):
#     """
#     ê° í´ë˜ìŠ¤ì˜ ì‘ì§‘ë„(compactness) ì¸¡ì •

#     Args:
#         embeddings: torch.Tensor [N, D]
#         labels: torch.Tensor [N]

#     Returns:
#         list of dict with class-wise compactness metrics
#     """
#     embeddings_np = embeddings.cpu().numpy()
#     labels_np = labels.cpu().numpy()

#     unique_labels = np.unique(labels_np)
#     class_variances = []

#     for label in unique_labels:
#         class_mask = labels_np == label
#         class_embeddings = embeddings_np[class_mask]

#         if len(class_embeddings) < 2:
#             continue

#         # í´ë˜ìŠ¤ ì¤‘ì‹¬
#         centroid = class_embeddings.mean(axis=0)

#         # ì¤‘ì‹¬ìœ¼ë¡œë¶€í„° í‰ê·  ê±°ë¦¬
#         distances = np.linalg.norm(class_embeddings - centroid, axis=1)
#         variance = distances.mean()

#         class_variances.append({
#             'class': int(label),
#             'compactness': float(variance),
#             'n_samples': len(class_embeddings)
#         })

#     return class_variances

# def evaluate_contrastive_quality(embeddings, labels, epoch, accelerator):
#     """
#     ëŒ€ì¡°í•™ìŠµ í’ˆì§ˆ ì¢…í•© í‰ê°€

#     Args:
#         embeddings: torch.Tensor [N, D] - Window-level embeddings
#         labels: torch.Tensor [N] - Window-level labels
#         epoch: int - Current epoch number
#         accelerator: Accelerator object

#     Returns:
#         dict with contrastive learning quality metrics
#     """
#     embeddings_np = embeddings.cpu().numpy()
#     labels_np = labels.cpu().numpy()

#     # 1. Silhouette Score (ì „ì²´ì ì¸ í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ)
#     try:
#         sil_score = silhouette_score(embeddings_np, labels_np, metric='cosine')
#     except Exception as e:
#         print(f"[Warning] Silhouette score computation failed: {e}")
#         sil_score = float('nan')

#     # 2. Intra/Inter distance ratio (ëŒ€ì¡°í•™ìŠµ íŠ¹í™”)
#     try:
#         contrastive_metrics = compute_contrastive_metrics(embeddings, labels)
#     except Exception as e:
#         print(f"[Warning] Contrastive metrics computation failed: {e}")
#         contrastive_metrics = {
#             'intra_dist': float('nan'),
#             'inter_dist': float('nan'),
#             'separation_ratio': float('nan')
#         }

#     # 3. Class compactness (í´ë˜ìŠ¤ë³„ ìƒì„¸ ë¶„ì„)
#     try:
#         class_stats = compute_class_compactness(embeddings, labels)
#     except Exception as e:
#         print(f"[Warning] Class compactness computation failed: {e}")
#         class_stats = []

#     if accelerator.is_main_process:
#         print(f"\n{'='*60}")
#         print(f"Epoch {epoch} - Contrastive Learning Quality Metrics")
#         print(f"{'='*60}")

#         # Silhouette Score
#         print(f"\nğŸ“Š Silhouette Score: {sil_score:.4f}")
#         if sil_score > 0.5:
#             status = "âœ… ìš°ìˆ˜ (Excellent separation)"
#         elif sil_score > 0.3:
#             status = "âœ… ì–‘í˜¸ (Good separation)"
#         elif sil_score > 0.2:
#             status = "âš ï¸  ë³´í†µ (Weak structure)"
#         else:
#             status = "âŒ ë¶ˆëŸ‰ (Poor separation)"
#         print(f"   Status: {status}")

#         # Contrastive Metrics
#         print(f"\nğŸ“ Distance Metrics:")
#         print(f"   Intra-class distance: {contrastive_metrics['intra_dist']:.4f} (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)")
#         print(f"   Inter-class distance: {contrastive_metrics['inter_dist']:.4f} (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)")
#         print(f"   Separation ratio: {contrastive_metrics['separation_ratio']:.4f}")

#         if contrastive_metrics['separation_ratio'] > 2.0:
#             sep_status = "âœ… ìš°ìˆ˜ (í´ë˜ìŠ¤ ê°„ ëª…í™•íˆ ë¶„ë¦¬ë¨)"
#         elif contrastive_metrics['separation_ratio'] > 1.5:
#             sep_status = "âœ… ì–‘í˜¸ (í´ë˜ìŠ¤ ê°„ ë¶„ë¦¬ ì§„í–‰ ì¤‘)"
#         elif contrastive_metrics['separation_ratio'] > 1.2:
#             sep_status = "âš ï¸  ë³´í†µ (ì•½í•œ ë¶„ë¦¬)"
#         else:
#             sep_status = "âŒ ë¶ˆëŸ‰ (í´ë˜ìŠ¤ê°€ ì„ì—¬ìˆìŒ)"
#         print(f"   Status: {sep_status}")

#         # Class-wise analysis
#         if len(class_stats) > 0:
#             print(f"\nğŸ“¦ Class-wise Compactness:")
#             for stats in class_stats:
#                 print(f"   Class {stats['class']}: "
#                       f"compactness={stats['compactness']:.4f}, "
#                       f"n={stats['n_samples']}")

#         # ì¢…í•© íŒë‹¨
#         print(f"\n{'='*60}")
#         print(f"ğŸ’¡ ì¢…í•© íŒë‹¨:")

#         if sil_score > 0.3 or contrastive_metrics['separation_ratio'] > 1.5:
#             print(f"   âœ… ëŒ€ì¡°í•™ìŠµì´ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê³„ì† í•™ìŠµí•˜ì„¸ìš”.")
#         elif sil_score > 0.2 or contrastive_metrics['separation_ratio'] > 1.2:
#             print(f"   âš ï¸  ì•½í•œ í´ëŸ¬ìŠ¤í„° í˜•ì„±. 5-10 epoch ë” ì§€ì¼œë³´ì„¸ìš”.")
#         else:
#             print(f"   âŒ í´ëŸ¬ìŠ¤í„°ê°€ í˜•ì„±ë˜ì§€ ì•ŠìŒ. ì „ëµ ë³€ê²½ ê³ ë ¤:")
#             print(f"      1. UCL ë¹„ì¤‘ ì¤„ì´ê¸° (ucl_weight ê°ì†Œ)")
#             print(f"      2. Supervised-onlyë¡œ ì „í™˜ (UCL ì œê±°)")
#             print(f"      3. Projection head ì°¨ì› ë” ëŠ˜ë¦¬ê¸° (512â†’768)")
#         print(f"{'='*60}\n")

#     return {
#         'silhouette': sil_score,
#         'intra_dist': contrastive_metrics['intra_dist'],
#         'inter_dist': contrastive_metrics['inter_dist'],
#         'separation_ratio': contrastive_metrics['separation_ratio'],
#         'class_stats': class_stats
#     }


# def prepare_multiview_inputs(batch, device, has_cxr, has_text, num_views, dataset, validation_mode=False, img_augmentor=None, disable_cxr=False, disable_txt=False, max_length=256, valid_seq_mask=None, accelerator=None, disable_augmentation=False):
#     ts = batch['ts_tensor']
#     observed_mask = batch['observed_mask']  # [B, W, T, D] - 1=ê´€ì¸¡, 0=ë³´ê°„/ê²°ì¸¡
#     img_index_tensor = batch['img_index_tensor']
#     txt_index_tensor = batch['text_index_tensor']
#     unique_img_paths = batch['unique_img_paths']   # list[str]
#     unique_txt_keys = batch['unique_txt_keys']     # list[(stay_id, hour_slot)]

#     B, W, T, D = ts.shape
#     V = 1 if (validation_mode or disable_augmentation) else num_views

#     has_cxr = has_cxr.unsqueeze(1).expand(-1, V, -1, -1)  # [B, V, W, T]
#     has_text = has_text.unsqueeze(1).expand(-1, V, -1, -1)  # [B, V, W, T]

#     ts_views, cxr_views, text_ids_views = [], [], []
#     aug_masks = []  # Augmentation mask tracking

#     # ==================== TEXT PREPARATION ====================
#     if not disable_txt:
#         text_ids_list, text_masks_list = [], []
#         for stay_id, hour in unique_txt_keys:
#             token = dataset.text_map[stay_id][hour]
#             ids = torch.tensor(token['input_ids'], dtype=torch.long)
#             mask = torch.tensor(token['attention_mask'], dtype=torch.long)

#             # max_lengthì— ë§ì¶° padding / truncation
#             current_len = len(ids)
#             if current_len > max_length:
#                 ids = ids[:max_length]
#                 mask = mask[:max_length]
#             elif current_len < max_length:
#                 pad_len = max_length - current_len
#                 ids = F.pad(ids, (0, pad_len), value=0)
#                 mask = F.pad(mask, (0, pad_len), value=0)

#             text_ids_list.append(ids)
#             text_masks_list.append(mask)

#         if len(unique_txt_keys) > 0:
#             unique_text_ids = torch.stack(text_ids_list, dim=0).to(device, non_blocking=True)  # [N_unique, max_length]
#             unique_text_masks = torch.stack(text_masks_list, dim=0).to(device, non_blocking=True)
#         else:
#             unique_text_ids = torch.empty(0, max_length, dtype=torch.long, device=device)
#             unique_text_masks = torch.empty(0, max_length, dtype=torch.long, device=device)

#     else:
#         # í…ìŠ¤íŠ¸ ëª¨ë‹¬ë¦¬í‹° ë¹„í™œì„±í™” ì‹œ, ë¹ˆ í…ì„œë¡œ ì´ˆê¸°í™”
#         unique_text_ids = torch.empty(0, max_length, dtype=torch.long, device=device)
#         unique_text_masks = torch.empty(0, max_length, dtype=torch.long, device=device)

#     # ==================== IMG PREPARATION ====================
#     if not disable_cxr:
#         if len(unique_img_paths) > 0 :
#             unique_imgs = torch.stack(
#                 [dataset.load_image_cached(path) for path in unique_img_paths],
#                 dim=0
#             ).to(device, non_blocking=True)
#         else: 
#             unique_imgs = torch.empty(0, 3, 224, 224, device=device)
#     else: 
#         # ì´ë¯¸ì§€ ëª¨ë‹¬ë¦¬í‹° ë¹„í™œì„±í™” ì‹œ, ë¹ˆ í…ì„œë¡œ ì´ˆê¸°í™”
#         unique_imgs = torch.empty(0, 3, 224, 224, device=device)

#     for v in range(V):
#         # ===============================================
#         # 1) Time-series augmentation
#         # ===============================================
#         with timer("TS augmentation", accelerator):
#             ts_v = ts.clone()
#             aug_mask_v = None
#             if v == 1 and not validation_mode and not disable_augmentation:
#                 ts_v, aug_mask_v = augment_time_series(
#                     ts_v,
#                     valid_seq_mask=valid_seq_mask,  # [B, W, T]: ìœ íš¨í•œ timestepë§Œ ì¦ê°•
#                     end_mask_steps=0,
#                     random_mask_ratio=0.1,  # ìœ íš¨í•œ timestepì˜ 10% ëœë¤ ë§ˆìŠ¤í‚¹
#                     feature_mask_prob=0.2,
#                     noise_scale=0.05
#                 )
#             else:
#                 # Validation/view 0: no augmentation
#                 aug_mask_v = torch.ones_like(ts_v)  # ëª¨ë“  ê°’ì´ ì¡´ì¬

#             ts_views.append(ts_v.to(device, non_blocking=True))
#             aug_masks.append(aug_mask_v)

#         # ===============================================
#         # 2) Text augmentation
#         # ===============================================
#         with timer("Text augmentation", accelerator):
#             if not disable_txt:
#                 ids_v = unique_text_ids.clone()  # [N_unique, max_length]
#                 mask_v = unique_text_masks.clone()

#                 if v == 1 and not validation_mode and not disable_augmentation:
#                     for i in range(len(unique_txt_keys)):
#                         new_ids, new_mask = delete_random_tokens(
#                             ids_v[i], mask_v[i],
#                             delete_prob=0.15,
#                             special_token_ids=[101, 102],  # BioBERT [CLS], [SEP]
#                             max_length=max_length
#                         )
#                         ids_v[i] = new_ids
#                         mask_v[i] = new_mask

#                 # Unique í…ìŠ¤íŠ¸ + scatter ì¸ë±ìŠ¤ ì €ì¥
#                 valid_positions = (txt_index_tensor != -1).nonzero(as_tuple=False)  # [N_valid, 3] (b,w,t)

#                 if len(valid_positions) > 0:
#                     # ìœ íš¨í•œ ìœ„ì¹˜ì˜ unique index ì¶”ì¶œ (scatterìš©)
#                     unique_indices = txt_index_tensor[valid_positions[:, 0], valid_positions[:, 1], valid_positions[:, 2]]

#                     text_ids_views.append({
#                         'unique_input_ids': ids_v,           # [N_unique, max_length]
#                         'unique_attention_mask': mask_v,     # [N_unique, max_length]
#                         'unique_indices': unique_indices,    # [N_valid] â† Scatter ì¸ë±ìŠ¤
#                         'positions': valid_positions         # [N_valid, 3] with (b, w, t)
#                     })
#                 else:
#                     # ì´ viewì— ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ê²½ìš°
#                     text_ids_views.append({
#                         'unique_input_ids': torch.empty(0, max_length, dtype=torch.long, device=device),
#                         'unique_attention_mask': torch.empty(0, max_length, dtype=torch.long, device=device),
#                         'unique_indices': torch.empty(0, dtype=torch.long, device=device),
#                         'positions': torch.empty(0, 3, dtype=torch.long, device=device)
#                     })

#             else:
#                 # Text ëª¨ë‹¬ë¦¬í‹° ë¹„í™œì„±í™” ì‹œ
#                 text_ids_views.append({
#                     'unique_input_ids': torch.empty(0, max_length, dtype=torch.long, device=device),
#                     'unique_attention_mask': torch.empty(0, max_length, dtype=torch.long, device=device),
#                     'unique_indices': torch.empty(0, dtype=torch.long, device=device),
#                     'positions': torch.empty(0, 3, dtype=torch.long, device=device)
#                 })

#         # ===============================================
#         # 3) CXR augmentation
#         # ===============================================
#         with timer("Img augmentation", accelerator):
#             if not disable_cxr:
#                 cxr_v = unique_imgs.clone()
#                 if v == 1 and not validation_mode and img_augmentor is not None and not disable_augmentation:
#                     cxr_v = img_augmentor(cxr_v)  # N_uniqueë§Œ ì¦ê°•

#                 # Unique ì´ë¯¸ì§€ + scatter ì¸ë±ìŠ¤ ì €ì¥
#                 valid_positions = (img_index_tensor != -1).nonzero(as_tuple=False)  # [N_valid, 3] (b,w,t)

#                 if len(valid_positions) > 0:
#                     # ìœ íš¨í•œ ìœ„ì¹˜ì˜ unique index ì¶”ì¶œ (scatterìš©)
#                     unique_indices = img_index_tensor[valid_positions[:, 0], valid_positions[:, 1], valid_positions[:, 2]]

#                     cxr_views.append({
#                         'unique_images': cxr_v,              # [N_unique, 3, 224, 224] â† Uniqueë§Œ!
#                         'unique_indices': unique_indices,    # [N_valid] â† Scatter ì¸ë±ìŠ¤
#                         'positions': valid_positions         # [N_valid, 3] with (b, w, t)
#                     })
#                 else:
#                     # ì´ viewì— ìœ íš¨í•œ ì´ë¯¸ì§€ê°€ ì—†ëŠ” ê²½ìš°
#                     cxr_views.append({
#                         'unique_images': torch.empty(0, 3, 224, 224, device=device),
#                         'unique_indices': torch.empty(0, dtype=torch.long, device=device),
#                         'positions': torch.empty(0, 3, dtype=torch.long, device=device)
#                     })

#             else:
#                 # CXR ëª¨ë‹¬ë¦¬í‹° ë¹„í™œì„±í™” ì‹œ
#                 cxr_views.append({
#                     'unique_images': torch.empty(0, 3, 224, 224, device=device),
#                     'unique_indices': torch.empty(0, dtype=torch.long, device=device),
#                     'positions': torch.empty(0, 3, dtype=torch.long, device=device)
#                 })

#     ts_series = torch.stack(ts_views, dim=1)  # [B, V, W, T, D]
#     aug_masks = torch.stack(aug_masks, dim=1)  # [B, V, W, T, D]

#     # ==================== Effective Mask ê³„ì‚° ====================
#     # effective_mask = observed_mask * aug_mask
#     # 1 = ê´€ì¸¡ë˜ê³  ì¦ê°• ë¯¸ì ìš© (ì‹ ë¢° ê°€ëŠ¥)
#     # 0 = ë³´ê°„/ê²°ì¸¡ì´ê±°ë‚˜ ì¦ê°•ë¨ (ì‹ ë¢° ë¶ˆê°€)
#     # observed_mask: 1=ê´€ì¸¡, 0=ë³´ê°„/ê²°ì¸¡
#     # aug_mask: 1=ê°’ì´ ì¡´ì¬, 0=ê°’ì´ ì œê±°ë¨ (augmentationìœ¼ë¡œ ë§ˆìŠ¤í‚¹ë¨)
#     observed_mask_expanded = observed_mask.unsqueeze(1).repeat(1, V, 1, 1, 1)  # [B, V, W, T, D]
#     effective_mask = observed_mask_expanded * aug_masks

#     # =======================================================
#     # total_img_positions = sum(len(view['positions']) for view in cxr_views) if not disable_cxr else 0
#     # total_txt_positions = sum(len(view['positions']) for view in text_ids_views) if not disable_txt else 0

#     # print(f"[prepare_multiview] B={B}, W={W}, T={T}, V={V}")
#         # f"Unique imgs={len(unique_img_paths)}, Positions={total_img_positions} ({total_img_positions/len(unique_img_paths) if len(unique_img_paths)>0 else 0:.1f}x) | "
#         # f"Unique txts={len(unique_txt_keys)}, Positions={total_txt_positions} ({total_txt_positions/len(unique_txt_keys) if len(unique_txt_keys)>0 else 0:.1f}x)")
#     # =======================================================
#     return ts_series, cxr_views, text_ids_views, has_cxr, has_text, effective_mask