import os
from contextlib import contextmanager
import time
import random
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

import torch
import torch.distributed as dist

from training.run import parse_arguments

# TIMER ì „ì—­ ì œì–´
TIME_ENABLED = False # True: ì‘ë™ / False: ë¯¸ì‘ë™

def plot_latent_time_attention(attn, save_path=None):
    B, L, T = attn.shape

    fig, axes = plt.subplots(
        nrows=B,
        ncols=1,
        figsize=(12, 2.5 * B),
        sharex=True
    )

    if B == 1:
        axes = [axes]

    for b in range(B):
        sns.heatmap(
            attn[b].cpu().numpy(),
            ax=axes[b],
            cmap="coolwarm",
            cbar=(b == 0),
            xticklabels=range(T),
            yticklabels=[f"latent {i}" for i in range(L)]
        )
        # axes[b].set_ylabel(f"sample {b}")

    axes[-1].set_xlabel("ICU Window Time Steps")
    plt.tight_layout()

    if save_path is not None:
        plt.savefig(save_path, dpi=200)
        plt.close()
    else:
        plt.show()



# SEED CONTROL
def set_seed(seed: int = 42):
    """
    ì¬í˜„ì„±ì„ ìœ„í•´ Python, NumPy, PyTorchì˜ ì‹œë“œë¥¼ ê³ ì •í•˜ëŠ” í•¨ìˆ˜
    """
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # ë©€í‹°-GPU ì‚¬ìš© ì‹œ
    np.random.seed(seed)
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    
    # CUDNN deterministic ì„¤ì •
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    print(f"ğŸ¯ [SEED] All seeds set to {seed} for reproducibility")
    

def seed_worker(worker_id):
    """
    DataLoaderì˜ ë©€í‹°-í”„ë¡œì„¸ì‹± ì›Œì»¤ë“¤ì˜ ì‹œë“œë¥¼ ê³ ì •í•˜ëŠ” í•¨ìˆ˜
    """
    worker_seed = torch.initial_seed() % 2**32
    np.random.seed(worker_seed)
    random.seed(worker_seed)

@contextmanager
def timer(name, accelerator=None):
    """
    Context manager for timing code execution.
    Only prints on main process when accelerator is provided.

    Args:
        name: Description of the timed block
        accelerator: Accelerate accelerator instance (optional)
    """
    if not TIME_ENABLED:
        yield
        return

    start = time.perf_counter()
    yield
    end = time.perf_counter()

    # Only log on main process
    if accelerator is not None:
        if accelerator.is_main_process:
            print(f"[Timer] {name}: {end - start:.4f}s")
    else:
        # Fallback to distributed rank check
        rank = dist.get_rank() if dist.is_initialized() else 0
        if rank == 0:
            print(f"[Timer] {name}: {end - start:.4f}s")


def log_memory(tag=""):
    """
    Allocated: ì‹¤ì œë¡œ ëª¨ë¸ì´ë‚˜ ì—°ì‚°ì— ì‚¬ìš© ì¤‘ì¸ ë©”ëª¨ë¦¬ ì–‘ (=í˜„ì¬ GPUì—ì„œ pytorch tensorê°€ ì°¨ì§€í•˜ëŠ” ë©”ëª¨ë¦¬ ì´ëŸ‰)
    Reserved: Pytorch ë©”ëª¨ë¦¬í’€ì— ì˜í•´ ì˜ˆì•½ëœ ë©”ëª¨ë¦¬ ì–‘
    """
    allocated = torch.cuda.memory_allocated() / (1024 ** 2)
    reserved = torch.cuda.memory_reserved() / (1024 ** 2)

    free_mem, total_mem = torch.cuda.mem_get_info()          # bytes
    free_mem_mb = free_mem / (1024 ** 2)
    total_mem_mb = total_mem / (1024 ** 2)
    used_mem_mb = total_mem_mb - free_mem_mb
    used_ratio = used_mem_mb / total_mem_mb * 100

    print(f"[GPU MEM - {tag}]")
    print(f"  Allocated: {allocated:.2f} MB")
    print(f"  Reserved : {reserved:.2f} MB")
    print(f"  Used     : {used_mem_mb:.2f} MB / {total_mem_mb:.2f} MB ({used_ratio:.1f}%)")
    print(f"  Free     : {free_mem_mb:.2f} MB")


def mask_tokenized_inputs(input_ids_tensor, attention_mask_tensor, mask_token_id=103, mask_prob=0.2, special_token_ids=[101, 102]):
    """
    101 = [CLS], 102 = [SEP], 103 = [MASK]
    """

    special_token_ids_tensor = torch.tensor(special_token_ids, dtype=input_ids_tensor.dtype, device=input_ids_tensor.device)
    is_special_token = torch.isin(input_ids_tensor, special_token_ids_tensor)
    is_padding = attention_mask_tensor == 0

    cannot_mask = is_special_token | is_padding
    random_mask = torch.rand_like(input_ids_tensor.float()) < mask_prob
    mask = random_mask & (~cannot_mask)

    masked_input_ids = input_ids_tensor.clone()
    masked_input_ids[mask] = mask_token_id
    return masked_input_ids


def padding_text(x, token_max_length=256):
    x = list(x)
    x = x[:token_max_length]
    if len(x) < token_max_length:
        x += [0] * (token_max_length - len(x))
    return x


def get_temperature(epoch, total_epochs, args):
    """
    decay: Trueì¼ ê²½ìš° temperatureë¥¼ ì¡°ì •í•˜ê³ , Falseì¼ ê²½ìš° ê³ ì •ê°’ ìœ ì§€
    """
    if args.use_temp_decay:
        return args.init_temp * ((args.final_temp / args.init_temp) ** (epoch / total_epochs))
    else:
        return args.fixed_temp


def debug_tensor(name, tensor, print_stats=False): 
    if torch.isnan(tensor).any():
        print(f"[DEBUG] NaN detected in {name}")
    if torch.isinf(tensor).any():
        print(f"[DEBUG] Inf detected in {name}")
    if print_stats:
        print(f"[DEBUG] {name} mean: {tensor.mean().item():.4f}, std: {tensor.std().item():.4f}")


class Earlystopping:
    def __init__(self, patience, start_epoch=0, save_path=None, experiment_id=None):
        self.patience = patience
        self.start_epoch = start_epoch
        self.best_auroc = float('-inf')
        self.counter = 0
        self.experiment_id = experiment_id

        # experiment_idê°€ ì œê³µë˜ë©´ ì‹¤í—˜ë³„ í´ë” ìƒì„±
        if save_path is not None and experiment_id is not None:
            base_dir = os.path.dirname(save_path)
            filename = os.path.basename(save_path)
            self.save_path = os.path.join(base_dir, filename)
        else:
            self.save_path = save_path

    def __call__(self, args, auroc, model, epoch):
        early_stop = False

        # early stopping ì‹œì‘ ì‹œì ì€ warm up ì¢…ë£Œ ì‹œì 
        if epoch < self.start_epoch:
            return False

        if auroc > self.best_auroc:
            self.best_auroc = auroc
            self.counter = 0

            if self.save_path is not None:
                os.makedirs(os.path.dirname(self.save_path), exist_ok=True)
                torch.save(model.state_dict(), self.save_path)
                print(f"[Epoch {epoch+1}] ğŸ”¥ ì„±ëŠ¥ í–¥ìƒ! Best AUROC: {self.best_auroc:.4f} ([ê³µí†µ ê²½ë¡œì— ë®ì–´ì”Œì›ë‹ˆë‹¤.]: {self.save_path})")
        else:
            self.counter += 1
            print(f"ğŸ“‰ ì„±ëŠ¥ ê°œì„  ì—†ìŒ. patience ì¶”ê°€ {self.counter}")

            if self.counter >= self.patience:
                early_stop = True
        return early_stop

    def get_best_model_path(self):
        """í•™ìŠµ ì¢…ë£Œ í›„ best model ê²½ë¡œ ë°˜í™˜"""
        return self.save_path

class stage2_Earlystopping:
    def __init__(self, patience, start_epoch=0, save_path=None, experiment_id=None):
        self.patience = patience
        self.start_epoch = start_epoch
        self.best_auroc = float('-inf')
        self.counter = 0
        self.experiment_id = experiment_id

        # experiment_idê°€ ì œê³µë˜ë©´ ì‹¤í—˜ë³„ í´ë” ìƒì„±
        if save_path is not None and experiment_id is not None:
            base_dir = os.path.dirname(save_path)
            filename = os.path.basename(save_path)
            self.save_path = os.path.join(base_dir, f"experiment_{experiment_id}", filename)
        else:
            self.save_path = save_path

    def __call__(self, args, auroc, model, epoch):
        early_stop = False

        # early stopping ì‹œì‘ ì‹œì ì€ warm up ì¢…ë£Œ ì‹œì 
        if epoch < self.start_epoch:
            return False

        if auroc > self.best_auroc:
            self.best_auroc = auroc
            self.counter = 0

            # ì‹¤í—˜ë³„ í´ë”ì— ì €ì¥
            if self.save_path is not None:
                os.makedirs(os.path.dirname(self.save_path), exist_ok=True)
                torch.save(model.state_dict(), self.save_path)
                print(f"[Epoch {epoch+1}] ğŸ”¥ ì„±ëŠ¥ í–¥ìƒ! Best AUROC: {self.best_auroc:.4f} (ì €ì¥ ì™„ë£Œ: {self.save_path})")
        else:
            self.counter += 1
            print(f"ğŸ“‰ ì„±ëŠ¥ ê°œì„  ì—†ìŒ. patience ì¶”ê°€ {self.counter}")

            if self.counter >= self.patience:
                early_stop = True
        return early_stop

    def get_best_model_path(self):
        """í•™ìŠµ ì¢…ë£Œ í›„ best model ê²½ë¡œ ë°˜í™˜"""
        return self.save_path


class stage1_earlystopping:
    """
    Stage 1ìš© Early Stopping: Loss ê¸°ë°˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
    """
    def __init__(self, patience, start_epoch=0):
        self.patience = patience
        self.start_epoch = start_epoch
        self.best_loss = float('inf')  # ë‚®ì„ìˆ˜ë¡ ì¢‹ìœ¼ë¯€ë¡œ infë¡œ ì´ˆê¸°í™”
        self.counter = 0

    def __call__(self, args, loss, model, epoch):
        """
        Args:
            loss: ì‹¤ì œ loss ê°’ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        """
        early_stop = False

        # early stopping ì‹œì‘ ì‹œì ì€ warm up ì¢…ë£Œ ì‹œì 
        if epoch < self.start_epoch:
            return False

        if loss < self.best_loss:  # Lossê°€ ê°ì†Œí•˜ë©´ ê°œì„ 
            self.best_loss = loss
            self.counter = 0
            print(f"[Epoch {epoch+1}] ğŸ”¥ Loss ê°œì„ ! (Best Loss: {self.best_loss:.4f})")
        else:
            self.counter += 1
            print(f"ğŸ“‰ Loss ê°œì„  ì—†ìŒ. patience ì¶”ê°€ {self.counter} (Current: {loss:.4f}, Best: {self.best_loss:.4f})")

            if self.counter >= self.patience:
                early_stop = True
        return early_stop


def compute_class_weights(label_tensor, num_classes):
    counts = torch.bincount(label_tensor[label_tensor != -1], minlength=num_classes).float()
    total = counts.sum()
    weights = total / (counts + 1e-8)  # Avoid division by zero
    weights = weights / weights.sum() * num_classes  # Optional normalization
    return weights


def debug_unique_batches_gpu(loader, accelerator):
    """
    ê° rankê°€ ë°›ì€ stay_id ì§‘í•©ì˜ êµì§‘í•©ì„ í™•ì¸í•´
    ì¤‘ë³µ ë°°ì¹˜ê°€ ì—†ëŠ”ì§€ ê²€ì¦í•œë‹¤. êµì§‘í•© í¬ê¸°ê°€ 0ì´ë©´ OK.
    """
    # 1) ë¡œì»¬ id ìˆ˜ì§‘
    local_ids = []
    for batch in loader:
        local_ids.extend(batch['stay_ids'])
    local_ids = torch.tensor(list(set(local_ids)), device=accelerator.device)

    # 2) rank ê°„ gather
    gathered = accelerator.gather(local_ids)
    if accelerator.is_main_process:
        sets = [set(t.tolist()) for t in gathered]
        inter = set.intersection(*sets)
        print(f"[DEBUG] ğŸŸ¢ intersection size = {len(inter)} (0ì´ë©´ ì¤‘ë³µ ì—†ìŒ ì˜ë¯¸í•¨.)")


def print_modality_stats(name, tensor):
    print(f"[{name}] mean: {tensor.abs().mean().item():.4e}, max: {tensor.abs().max().item():.4e}")


# ==================== GRADIENT AMPLIFICATION FOR SELECTIVE LAYERS ====================

def DEFINE_LAYER_GROUP(model, layer_patterns=None):
    """
    ëª¨ë¸ì—ì„œ ì§€ì •ëœ íŒ¨í„´ì˜ ëª¨ë“ˆì„ ì°¾ì•„ì„œ ê·¸ë£¹ Gë¡œ ì •ì˜ (ëª¨ë“ˆ ë ˆë²¨)
    ì„ íƒëœ ëª¨ë“ˆì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ê°€ í•¨ê»˜ ì¦í­ë¨
    ë‚˜ì¤‘ì— ë‹¤ë¥¸ ë ˆì´ì–´ íŒ¨í„´ë„ ì¶”ê°€ ê°€ëŠ¥í•œ êµ¬ì¡°

    Args:
        model: í›ˆë ¨ ì¤‘ì¸ ëª¨ë¸ (Acceleratorë¡œ wrapping ê°€ëŠ¥)
        layer_patterns: ì°¾ì„ ëª¨ë“ˆ ì´ë¦„ íŒ¨í„´ ë¦¬ìŠ¤íŠ¸
        ê¸°ë³¸ê°’: ['cxr_cross', 'text_cross']

    Returns:
        G: ì„ íƒëœ ëª¨ë“ˆ ê°ì²´ ë¦¬ìŠ¤íŠ¸ (in_proj, out_proj ë“± í¬í•¨)
        G_names: í•´ë‹¹ ëª¨ë“ˆì˜ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (ë””ë²„ê¹…ìš©)
    """
    if layer_patterns is None:
        layer_patterns = ['cxr_cross', 'text_cross']

    G = []
    G_names = []

    unwrapped_model = getattr(model, "module", model)

    for name, module in unwrapped_model.named_modules():
        # íŒ¨í„´ê³¼ ì¼ì¹˜í•˜ê³  íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§„ ëª¨ë“ˆ ì°¾ê¸°
        if any(pattern in name for pattern in layer_patterns):
            # íŒŒë¼ë¯¸í„°ê°€ ìˆëŠ” ëª¨ë“ˆë§Œ ì„ íƒ
            if sum(p.numel() for p in module.parameters()) > 0:
                G.append(module)
                G_names.append(name)

    # print(f"ğŸ¯ [Layer Group] Found {len(G)} modules with patterns {layer_patterns}:")
    # for name in sorted(G_names):
    #     print(f"   - {name}")

    return G, G_names


def GRADIENT_AMPLIFICATION(G, G_names, alpha=3.0, exclude_patterns=None):
    """
    ì§€ì •ëœ ëª¨ë“ˆì˜ íŒŒë¼ë¯¸í„° ê·¸ë˜ë””ì–¸íŠ¸ì— alphaë¥¼ ê³±í•˜ì—¬ ì¦í­
    ì œì™¸í•  íŒŒë¼ë¯¸í„° íŒ¨í„´ì„ ìœ ì—°í•˜ê²Œ ì§€ì • ê°€ëŠ¥

    Args:
        G: ì„ íƒëœ ëª¨ë“ˆ ê°ì²´ ë¦¬ìŠ¤íŠ¸
        G_names: í•´ë‹¹ ëª¨ë“ˆì˜ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (ë””ë²„ê¹…ìš©)
        alpha: ê·¸ë˜ë””ì–¸íŠ¸ ì¦í­ ë°°ìˆ˜ (default: 3.0)
        exclude_patterns: ì œì™¸í•  íŒŒë¼ë¯¸í„° ì´ë¦„ íŒ¨í„´ ë¦¬ìŠ¤íŠ¸
    """
    if not G:
        return

    if exclude_patterns is None:
        exclude_patterns = []

    amplified_count = 0
    skipped_count = 0
    total_grad_before = 0.0
    total_grad_after = 0.0

    for module, module_name in zip(G, G_names):
        module_params_count = 0
        module_skipped_count = 0

        for param_name, param in module.named_parameters():
            # ì œì™¸ íŒ¨í„´ í™•ì¸
            should_exclude = any(pattern in param_name for pattern in exclude_patterns)

            if should_exclude:
                skipped_count += 1
                module_skipped_count += 1
                continue

            if param.grad is not None:
                grad_norm_before = param.grad.norm().item()
                param.grad *= alpha
                grad_norm_after = param.grad.norm().item()

                total_grad_before += grad_norm_before
                total_grad_after += grad_norm_after
                amplified_count += 1
                module_params_count += 1

        if module_params_count > 0 or module_skipped_count > 0:
            skip_info = f" (excluded: {module_skipped_count})" if module_skipped_count > 0 else ""
            # print(f"   [AMP] {module_name}: {module_params_count} params amplified{skip_info}")

    # if amplified_count > 0:
        # print(f"[GRADIENT_AMPLIFICATION] Total {amplified_count} parameters (Î±={alpha:.1f}x)")
        # if exclude_patterns:
            # print(f"   Exclude patterns: {exclude_patterns} (skipped: {skipped_count})")
        # print(f"   Before: {total_grad_before:.4e} | After: {total_grad_after:.4e}")
    # else:
        # print("âš ï¸  Warning: No gradients found to amplify")